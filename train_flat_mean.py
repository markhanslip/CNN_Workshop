{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "from IPython.display import Image, display, HTML\n",
    "import os\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from functools import partial\n",
    "\n",
    "optimizer=None\n",
    "loss_fn=None\n",
    "model=None\n",
    "mean = None\n",
    "std = None\n",
    "batch_size=0\n",
    "train_data_loader=None\n",
    "valid_data_loader=None\n",
    "lr=0\n",
    "weight_decay=0\n",
    "model_name=''\n",
    "input_type=''\n",
    "train_loss_per_epoch=[]\n",
    "valid_loss_per_epoch=[]\n",
    "\n",
    "def load_data(data_path, batch_size):\n",
    "    global classes\n",
    "    global train_data_loader\n",
    "    global valid_data_loader\n",
    "    global mean\n",
    "    global std\n",
    "    global num_workers\n",
    "\n",
    "    input_res = 224\n",
    "    batch_size = opt.batch_size # 16 seems to work well\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(input_res),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5m 0.5, 0.5])\n",
    "    ])\n",
    "\n",
    "    full_dataset = torchvision.datasets.ImageFolder(root=data_path, transform=transform)\n",
    "    full_dataLoader = torch.utils.data.DataLoader(full_dataset, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    # if train and valid not pre-separated:\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    valid_size = len(full_dataset) - train_size\n",
    "\n",
    "    train_data, valid_data = torch.utils.data.random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    classes=full_dataLoader.dataset.classes\n",
    "    print('classes are '+str(classes))\n",
    "\n",
    "def load_model(model_name, device, optim_type, loss, lr, weight_decay):\n",
    "    global classes\n",
    "    global optimizer\n",
    "    global loss_fn\n",
    "    global model\n",
    "\n",
    "    if model_name == 'densenet121':\n",
    "        model = models.densenet121(pretrained=False, num_classes=len(classes))\n",
    "        print('model is densenet121')\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad=True\n",
    "\n",
    "    if model_name == 'resnet18':\n",
    "        model = models.resnet18(pretrained=False, num_classes=len(classes))\n",
    "        print('model is resnet18')\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad=True\n",
    "\n",
    "    if model_name == 'resnet34':\n",
    "        model = models.resnet34(pretrained=False, num_classes=len(classes))\n",
    "        print('model is resnet34')\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad=True\n",
    "\n",
    "    if optim_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay) # add more options here\n",
    "\n",
    "    if optim_type == 'adamw':\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    if loss == 'cross-entropy':\n",
    "        loss_fn = nn.CrossEntropyLoss() # add more options here\n",
    "\n",
    "    if device == 'cuda':\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"device is cuda\")\n",
    "    elif device == 'cpu':\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"device is cpu\")\n",
    "    model.to(device);\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_data_loader, valid_data_loader, epochs, device):\n",
    "\n",
    "    global train_loss_per_epoch\n",
    "    global valid_loss_per_epoch\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch += 1 # start at epoch 1 rather than 0\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        print('training epoch {}...'.format(epoch))\n",
    "\n",
    "        for batch in train_data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = batch\n",
    "            if device==\"cuda\":\n",
    "                inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            if device==\"cuda\":\n",
    "                labels = labels.to(device)\n",
    "            loss = loss_fn(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item()\n",
    "        training_loss /= len(train_data_loader)\n",
    "        print(\"epoch {} training done\".format(epoch))\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        num_correct = 0\n",
    "        num_examples = 0\n",
    "\n",
    "        print(\"about to run testing\")\n",
    "        for batch in valid_data_loader:\n",
    "            inputs, labels = batch\n",
    "            if device==\"cuda\":\n",
    "                inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            if device==\"cuda\":\n",
    "                labels = labels.to(device)\n",
    "            loss = loss_fn(output, labels)\n",
    "            valid_loss += loss.data.item()\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], labels).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "#             writer.add_scalar('accuracy', num_correct / num_examples, epoch)\n",
    "            #for i, m in enumerate(model.children()):\n",
    "            #    m.register_forward_hook(partial(send_stats, i))\n",
    "        valid_loss /= len(valid_data_loader)\n",
    "\n",
    "        train_loss_per_epoch.append(training_loss/len(train_data_loader))\n",
    "        valid_loss_per_epoch.append(valid_loss/len(valid_data_loader))\n",
    "\n",
    "        print(\"epoch: {}, training loss: {:.3f}, validation loss: {:.3f}, accuracy = {:.2f}\".format(epoch, training_loss, valid_loss, num_correct / num_examples))\n",
    "    print('Finished Training')\n",
    "\n",
    "def save_model(model_path, model_name):\n",
    "    global mean\n",
    "    global std\n",
    "\n",
    "    torch.save({\n",
    "    'model':model.state_dict(),\n",
    "    'classes':classes,\n",
    "    'resolution':224,\n",
    "    'modelType':model_name,\n",
    "    'mean':mean,\n",
    "    'std':std\n",
    "}, model_path)\n",
    "    \n",
    "    print('saved model file to '+model_path)\n",
    "\n",
    "def plot_training():\n",
    "\n",
    "    global train_loss_per_epoch\n",
    "    global valid_loss_per_epoch\n",
    "\n",
    "    plt.plot(train_loss_per_epoch, label='Training Loss', c='r')\n",
    "    plt.plot(valid_loss_per_epoch, label='Validation Loss', c='g')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def run_training(opt):\n",
    "    mean_std(opt.data_path)\n",
    "    load_data(opt.data_path, opt.batch_size)\n",
    "    load_model(opt.model_name, opt.device, opt.optim_type, opt.loss, opt.lr, opt.weight_decay)\n",
    "    train(model, optimizer, loss_fn, train_data_loader, valid_data_loader, opt.epochs, opt.device)\n",
    "    save_model(opt.model_path, opt.model_name)\n",
    "    plot_training()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_path', type=str, default='./data', help='path to folder containing 2 or more folders of spectrograms labelled by class')\n",
    "    parser.add_argument('--model_name', type=str, default='resnet18', help='choose from resnet18, resnet34, densenet121')\n",
    "    parser.add_argument('--device', type=str, help='cuda or cpu')\n",
    "    parser.add_argument('--optim_type', type=str, default='adam', help='choose from adam or adamw')\n",
    "    parser.add_argument('--loss', type=str, default='cross-entropy', help='cross-entropy only option for now')\n",
    "    parser.add_argument('--lr', type=float, help='learning rate')\n",
    "    parser.add_argument('--weight_decay', type=float, default=None, help='add L2 penalty, recommend for small datasets')\n",
    "    parser.add_argument('--epochs', type=int, help='number of epochs to train')\n",
    "    parser.add_argument('--model_path', type=str, help='path to model file will be saved, .pth extension')\n",
    "    parser.add_argument('--batch_size', type=int, help='size of minibatch, larger usually improves learning, lower saves memory')\n",
    "\n",
    "    opt = parser.parse_args()\n",
    "    run_training(opt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
